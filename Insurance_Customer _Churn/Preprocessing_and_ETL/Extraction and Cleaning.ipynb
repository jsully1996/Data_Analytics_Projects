{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql import SparkSession, functions, types\n",
    "#Create Spark Session and context\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"example code\")\\\n",
    "    .config(\"spark.driver.extraClassPath\",\"/home/jim/spark-2.4.0-bin-hadoop2.7/jars/mysql-connector-java-5.1.49.jar\")\\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "sc = spark.sparkContext\n",
    "#Recover stored Pandas Dataframe Objects\n",
    "%store -r df1\n",
    "%store -r fake_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas Dataframes to Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Dataframe:\n",
      "+-----------+---------+-----------------------+---+--------+--------+---------+-----------------+-----------------+\n",
      "|Customer ID|     City|Customer Lifetime Value|Age|Response|Coverage|Education|Effective To Date|Employment_Status|\n",
      "+-----------+---------+-----------------------+---+--------+--------+---------+-----------------+-----------------+\n",
      "|    DA29911|   Bhopal|     1339.3965560000001| 32|      No|Extended|Grade XII|       07-12-2020|       Unemployed|\n",
      "|    PL40163|  Kolkata|            6388.288678| 34|      No|   Basic|  Diploma|       05-03-2019|       Unemployed|\n",
      "|    CS14065|  Chennai|            6077.675806| 49|     Yes| Premium|  Diploma|       06-05-2020|          Retired|\n",
      "|    KJ49509|  Kolkata|     3615.7581549999995| 40|     Yes| Premium|  Grade X|       01-06-2018|       Unemployed|\n",
      "|    CF57721|   Bhopal|            5015.968678| 47|     Yes| Premium| Graduate|       30-10-2020|         On leave|\n",
      "|    BU35480|     Pune|            2928.508467| 46|      No| Premium|  Diploma|       31-07-2018|       Unemployed|\n",
      "|    UK89345|     Pune|            5237.849466| 48|     Yes| Premium| Graduate|       28-03-2019|         Employed|\n",
      "|    ZR36845|  Kolkata|            3269.851544| 47|      No| Premium| Graduate|       30-07-2018|          Retired|\n",
      "|    NC26827|    Delhi|             1318.01513| 26|     Yes|Extended|   Master|       08-09-2018|         Employed|\n",
      "|    WY87597|Bengaluru|            9564.454456| 31|      No|   Basic|   Master|       24-07-2020|         On leave|\n",
      "+-----------+---------+-----------------------+---+--------+--------+---------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Second Dataframe:\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Customer ID|                Name|             Address|            Phone_no|                 Job|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    JM74474|      Taylor Johnson|475 White Station...|  577.347.1095x72786|Lighting technici...|\n",
      "|    SB87239|      Isaiah Hammond|735 Joseph Lakes ...|    038.150.9094x021|Environmental hea...|\n",
      "|    OK68537|Michael Clements DVM|309 Leah Highway\n",
      "...|        792.000.7028|     Mining engineer|\n",
      "|    BR75237|     Regina Williams|97626 Mccall Terr...|    182.117.8878x877|       Media planner|\n",
      "|    QV59649|     Kathryn Maxwell|436 Martha Juncti...|     +1-834-389-3554|Engineer, communi...|\n",
      "|    NA70806|       Ronald Graham|40475 Nicole Shoa...|001-893-744-6464x...| Visual merchandiser|\n",
      "|    JV85293|      Audrey Shields|USCGC Williams\n",
      "FP...| (053)109-2244x97988|          IT trainer|\n",
      "|    UV74428|        Paul Ramirez|6008 Renee Cove\n",
      "E...|001-049-019-8601x...|Private music tea...|\n",
      "|    UN52068|  Michael Livingston|491 Jones Burg Ap...|  929-881-6565x77086|Engineer, technic...|\n",
      "|    RN95272|     Anthony Walters|37506 Joseph Clif...| (312)159-0511x79727|Scientist, audiol...|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df1 = spark.createDataFrame(df1)\n",
    "spark_df2 = spark.createDataFrame(fake_df)\n",
    "print('First Dataframe:')\n",
    "spark_df1.select(spark_df1.columns[:9]).show(10)\n",
    "print('\\nSecond Dataframe:')\n",
    "spark_df2.select(spark_df2.columns[:5]).show(10,truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the two SPark Dataframes, let us export their schema to a JSON file so that we can call upon it later if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema 1:\n",
      " struct<Customer ID:string,City:string,Customer Lifetime Value:double,Age:bigint,Response:string,Coverage:string,Education:string,Effective To Date:string,Employment_Status:string,Gender:string,Income:bigint,Location_Code:string,Marital Status:string,Monthly Premium Auto:bigint,Total Written Premium:bigint,Losses:bigint,Loss Ratio:double,Growth Rate:double,Commissions:bigint,Months Since Last Claim:bigint,Months Since Policy Inception:bigint,Number of Open Complaints:bigint,Number of Policies:bigint,Number of previous policies:bigint,Policy_Type:string,Policy_Rating:string,Renew_Offer_Type:string,Sales_Channel:string,Total Claim Amount:double,Feedback:string>\n",
      "\n",
      "Schema 2:\n",
      " struct<Customer ID:string,Name:string,Address:string,Phone_no:string,Job:string,Company:string,Credit Card Provider:string,Email:string,SSN:string>\n"
     ]
    }
   ],
   "source": [
    "#Dump schemas of both the Dataframes to respective JSON files\n",
    "with open(\"schema_1.json\", \"w\") as f:\n",
    "    json.dump(spark_df1.schema.jsonValue(), f)\n",
    "    \n",
    "with open(\"schema_2.json\", \"w\") as f:\n",
    "    json.dump(spark_df2.schema.jsonValue(), f)\n",
    "    \n",
    "#Print the JSON file contents\n",
    "with open(\"schema_1.json\") as f:\n",
    "    print('Schema 1:\\n',types.StructType.fromJson(json.load(f)).simpleString())\n",
    "    \n",
    "with open(\"schema_2.json\") as f:\n",
    "    print('\\nSchema 2:\\n',types.StructType.fromJson(json.load(f)).simpleString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the Dataframes\n",
    "#### The Dataframes have a common column \"Customer ID\" which is also the primary key for both schemas\n",
    "#### Since Spark SQL supports native SQL syntax, we can also write join operations after creating temporary tables on DataFrameâ€™s and using spark.sql()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-----------------------+---+--------+--------+\n",
      "|Customer_ID|     City|Customer Lifetime Value|Age|Response|Coverage|\n",
      "+-----------+---------+-----------------------+---+--------+--------+\n",
      "|    CF15071|  Chennai|            5182.902158| 27|     Yes|   Basic|\n",
      "|    CP45498|     Pune|            2487.436701| 43|      No|   Basic|\n",
      "|    GH17700|  Kolkata|            6856.146035| 45|      No|   Basic|\n",
      "|    GR55787|Bengaluru|      7805.536112000001| 37|      No|   Basic|\n",
      "|    HO30596|   Bhopal|            2191.281335| 28|     Yes|   Basic|\n",
      "|    IG45345|     Pune|     2651.6145890000003| 50|      No|   Basic|\n",
      "|    JJ22950|  Chennai|      8931.096209000001| 30|     Yes|Extended|\n",
      "|    NM95686|Bengaluru|      9604.305604000001| 44|      No|Extended|\n",
      "|    NV68421|    Delhi|             9735.67953| 33|      No| Premium|\n",
      "|    OZ20388|Bengaluru|             8176.13623| 47|      No|Extended|\n",
      "+-----------+---------+-----------------------+---+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create Temp tables in SPark.sql\n",
    "spark_df1.withColumnRenamed(\"Customer ID\",\"Customer_ID\").createOrReplaceTempView(\"DF1\")\n",
    "spark_df2.withColumnRenamed(\"Customer ID\",\"Customer__ID\").createOrReplaceTempView(\"DF2\")\n",
    "\n",
    "#SQL JOIN\n",
    "joined_df = spark.sql(\"SELECT * FROM DF1 INNER JOIN DF2 ON DF1.Customer_ID = DF2.Customer__ID\")\n",
    "joined_df.select(joined_df.columns[:6]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint and cache() the dataframe so we spend less time performing computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Customer_ID: string, City: string, Customer Lifetime Value: double, Age: bigint, Response: string, Coverage: string, Education: string, Effective To Date: string, Employment_Status: string, Gender: string, Income: bigint, Location_Code: string, Marital Status: string, Monthly Premium Auto: bigint, Total Written Premium: bigint, Losses: bigint, Loss Ratio: double, Growth Rate: double, Commissions: bigint, Months Since Last Claim: bigint, Months Since Policy Inception: bigint, Number of Open Complaints: bigint, Number of Policies: bigint, Number of previous policies: bigint, Policy_Type: string, Policy_Rating: string, Renew_Offer_Type: string, Sales_Channel: string, Total Claim Amount: double, Feedback: string, Customer__ID: string, Name: string, Address: string, Phone_no: string, Job: string, Company: string, Credit Card Provider: string, Email: string, SSN: string]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master = joined_df\n",
    "joined_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "### Now we will apply the standard cleaning procedures:\n",
    "### 1.Dropping Rows With Empty Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows containing one or more null values were dropped\n"
     ]
    }
   ],
   "source": [
    "df_len = joined_df.count()\n",
    "joined_df = joined_df.dropna(how='any')\n",
    "print('{} rows containing one or more null values were dropped'.format(df_len-joined_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dropping Duplicate Rows i.e. duplicated entries/values\n",
    "#### Since our primary key is the Customer ID, we need to be sure that there is only one data entry for each CUstomer ID. In an ideal dataset, this step would drop zero rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 rows containing one or more null values were dropped\n"
     ]
    }
   ],
   "source": [
    "joined_df = joined_df.dropDuplicates(subset=[\"Customer_ID\"])\n",
    "print('{} rows containing one or more null values were dropped'.format(df_len-joined_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean Individual columns one by one\n",
    "#### Let us proceed from left to right. Customer lifetimevalue seems to have many decimals and they are varying. We can standardize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|Customer Lifetime Value|\n",
      "+-----------------------+\n",
      "|5182.9                 |\n",
      "|2487.44                |\n",
      "|6856.15                |\n",
      "|7805.54                |\n",
      "|2191.28                |\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Range of this columns is 1000.92 - 9998.96\n"
     ]
    }
   ],
   "source": [
    "#Round the number of decimals to 2\n",
    "joined_df = joined_df.withColumn(\"Customer Lifetime Value\", functions.round(\"Customer Lifetime Value\", 2))\n",
    "\n",
    "#SHow the result\n",
    "joined_df.select(\"Customer Lifetime Value\").show(5,truncate=False)\n",
    "\n",
    "#Print min and max values\n",
    "print('\\nRange of this columns is {} - {}'.format(joined_df.select('Customer Lifetime Value').rdd.min()[0],joined_df.select('Customer Lifetime Value').rdd.max()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will apply the same function to the columns \"Loss Ratio\",\"Growth Rate\" and \"Total Claim Amount\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------------------+\n",
      "|Loss Ratio|Growth Rate|Total Claim Amount|\n",
      "+----------+-----------+------------------+\n",
      "|0.734     |0.715      |39565.507         |\n",
      "|0.987     |7.916      |90278.463         |\n",
      "|0.977     |-7.184     |61447.61          |\n",
      "|0.735     |-3.985     |43840.157         |\n",
      "|0.134     |-0.794     |18663.599         |\n",
      "+----------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Range of Loss Ratio is 0.0 - 1.0\n",
      "Range of Growth Rate is -9.998 - 9.995\n",
      "Range of Total Claim Amount is 10023.563 - 99982.258\n"
     ]
    }
   ],
   "source": [
    "#Round the number of decimals to 2\n",
    "joined_df = joined_df.withColumn(\"Loss Ratio\", functions.round(\"Loss Ratio\", 3))\n",
    "joined_df = joined_df.withColumn(\"Growth Rate\", functions.round(\"Growth Rate\", 3))\n",
    "joined_df = joined_df.withColumn(\"Total Claim Amount\", functions.round(\"Total Claim Amount\", 3))\n",
    "\n",
    "#Show results\n",
    "joined_df.select(\"Loss Ratio\",\"Growth Rate\",\"Total Claim Amount\").show(5,truncate=False)\n",
    "\n",
    "#Print ranges\n",
    "for i in [\"Loss Ratio\",\"Growth Rate\",\"Total Claim Amount\"]:\n",
    "    print('Range of {} is {} - {}'.format(i,joined_df.select(i).rdd.min()[0],joined_df.select(i).rdd.max()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we shall clean the Address column\n",
    "Notice how the newline character seems to divide each entry into the house and town. We will just keep the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|Address                         |\n",
      "+--------------------------------+\n",
      "|40710 Christina Forest Suite 049|\n",
      "|689 Hughes Haven Apt. 941       |\n",
      "|PSC 3151, Box 0733              |\n",
      "|775 Dean Lights Suite 377       |\n",
      "|Unit 3938 Box 4045              |\n",
      "|Unit 8463 Box 7425              |\n",
      "|1596 Howe Field Apt. 622        |\n",
      "|997 Peterson Center Suite 651   |\n",
      "|2402 Cooper Neck Suite 618      |\n",
      "|78122 Sims Green                |\n",
      "+--------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = joined_df.withColumn(\"Address\",functions.split(\"Address\", \"\\n\").getItem(0))\n",
    "joined_df.select(\"Address\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the Job column is clean but sometimes there is too much information, separated by a comma. We can discard the second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|Job                     |\n",
      "+------------------------+\n",
      "|Insurance broker        |\n",
      "|Magazine features editor|\n",
      "|Scientist               |\n",
      "|Medical physicist       |\n",
      "|Best boy                |\n",
      "|Electronics engineer    |\n",
      "|Editor                  |\n",
      "|Ship broker             |\n",
      "|Herbalist               |\n",
      "|Horticulturist          |\n",
      "+------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = joined_df.withColumn(\"Job\",functions.split(\"Job\", \",\").getItem(0))\n",
    "joined_df.select(\"Job\").show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarly, we can do so for the \"Company\" column but here we must keep the second part. So if there is only one, part, the code must handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Company             |\n",
      "+--------------------+\n",
      "|Hoffman LLC         |\n",
      "| Ward and Rice      |\n",
      "|Johnson Inc         |\n",
      "| Dickerson and Bates|\n",
      "| Walton and Garcia  |\n",
      "|Armstrong and Sons  |\n",
      "|Irwin and Sons      |\n",
      "|Branch Group        |\n",
      "| Webb and Holloway  |\n",
      "| White and Curtis   |\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = joined_df.withColumn(\"Company\",functions.reverse(functions.split(\"Company\", \",\")).getItem(0))\n",
    "joined_df.select('Company').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Drop unnecessary columns \n",
    "#### While there is a lot of useful data, some of the columns are not required when it comes to using the data for churn prediction. Some of the unnecessary columns here are Customer_ID, Name, Address(not CIty), Phone number, Email and SSN as these fields are unlikely to affect a person's decision to buy insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer_ID',\n",
       " 'City',\n",
       " 'Customer Lifetime Value',\n",
       " 'Age',\n",
       " 'Response',\n",
       " 'Coverage',\n",
       " 'Education',\n",
       " 'Effective To Date',\n",
       " 'Employment_Status',\n",
       " 'Gender',\n",
       " 'Income',\n",
       " 'Location_Code',\n",
       " 'Marital Status',\n",
       " 'Monthly Premium Auto',\n",
       " 'Total Written Premium',\n",
       " 'Losses',\n",
       " 'Loss Ratio',\n",
       " 'Growth Rate',\n",
       " 'Commissions',\n",
       " 'Months Since Last Claim',\n",
       " 'Months Since Policy Inception',\n",
       " 'Number of Open Complaints',\n",
       " 'Number of Policies',\n",
       " 'Number of previous policies',\n",
       " 'Policy_Type',\n",
       " 'Policy_Rating',\n",
       " 'Renew_Offer_Type',\n",
       " 'Sales_Channel',\n",
       " 'Total Claim Amount',\n",
       " 'Feedback',\n",
       " 'Job',\n",
       " 'Company',\n",
       " 'Credit Card Provider']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = [\"Customer__ID\",\"Name\",\"Address\",\"Phone_no\",\"Email\",\"SSN\"]\n",
    "final_df = joined_df.select([C for C in joined_df.columns if C not in cols_to_drop])\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load into MySQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1841.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:193)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-e27475633540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"com.mysql.jdbc.Driver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Insurance_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jsully\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whatisreal1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1841.save.\n: java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions$$anonfun$5.apply(JDBCOptions.scala:99)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:99)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:193)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:45)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "final_df.write\\\n",
    "    .format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost/Insurance\")\\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "    .option(\"dbtable\", \"Insurance_data\").option(\"user\", \"jsully\")\\\n",
    "    .option(\"password\", \"whatisreal1\").mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.coalesce(1).write.format('csv').option('header',True).mode('overwrite').save('Insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
